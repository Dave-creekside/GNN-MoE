!pip install torch datasets numpy matplotlib seaborn
!pip uninstall -y datasets fsspec huggingface_hub transformers tokenizers
!rm -rf ~/.cache/huggingface/datasets
!pip install datasets==2.14.7 fsspec==2023.10.0 huggingface_hub==0.17.3 transformers==4.35.2 tokenizers==0.15.0
!pip install torch-geometric





#print(torch.__version__)
!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.6.0+cu124.html





!python run_gnn_moe.py \
  --coupler_type HGNN \
  --static_hyperedge_strategy all_pairs \
  --hgnn_learnable_edge_weights \
  --embed_dim 512 \
  --num_layers 8 \
  --batch_size 16 \
  --learning_rate 0.0005 \
  --gnn_layers 3 \
  --num_experts 2 \
  --epochs 10 \
  --eval_every 250 \
  --run_name "hgnn_vs_gnn_optimal_showdown" \
  --dataset_config_name wikitext-2-v1









# Ghost Expert Architecture Training
!python -m ghost.run_gnn_moe \
  --embed_dim 256 \
  --num_layers 4 \
  --num_experts 4 \
  --num_ghost_experts 4 \
  --ghost_activation_threshold 0.75 \
  --ghost_learning_rate 1e-4 \
  --epochs 3 \
  --max_batches_per_epoch 100 \
  --eval_every 50 \
  --run_name "ghost_notebook_demo" \
  --dataset_config_name wikitext-2-v1



# Test different ghost configurations
!python ghost/tests/run_ghost_sweep.py



# Test different architecture configurations
!python ghost/tests/run_architecture_sweep.py



import glob
import os

# Find latest sweep
sweep_dirs = glob.glob('ghost/tests/sweeps/*')
if sweep_dirs:
    latest_sweep = max(sweep_dirs, key=os.path.getctime)
    print(f"Analyzing: {latest_sweep}")
    !python ghost/tests/enhanced_analysis.py {latest_sweep}
else:
    print("No sweeps found")






import json
import pandas as pd
import matplotlib.pyplot as plt

# Load latest training log
log_files = glob.glob('ghost/tests/sweeps/*/*/training_log.json')
if log_files:
    latest_log = max(log_files, key=os.path.getctime)
    with open(latest_log, 'r') as f:
        training_data = json.load(f)
    
    df = pd.DataFrame(training_data)
    
    # Plot ghost activations over time
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))
    
    # Ghost activations
    ghost_activations = pd.DataFrame(df['ghost_activations'].tolist(), index=df['step'])
    ghost_activations.plot(ax=ax1, title='Ghost Expert Activation Levels')
    ax1.set_ylabel('Activation Level')
    
    # Saturation vs Orthogonality
    ax2.plot(df['step'], df['saturation_level'], label='Saturation', color='red')
    ax2_twin = ax2.twinx()
    ax2_twin.plot(df['step'], df['orthogonality_score'], label='Orthogonality', color='blue')
    ax2.set_title('Expert Dynamics')
    ax2.set_xlabel('Training Step')
    ax2.legend(loc='upper left')
    ax2_twin.legend(loc='upper right')
    
    plt.tight_layout()
    plt.show()
    
    print(f"Final ghost activations: {df.iloc[-1]['ghost_activations']}")
    print(f"Final saturation: {df.iloc[-1]['saturation_level']:.3f}")
else:
    print("No training logs found")




